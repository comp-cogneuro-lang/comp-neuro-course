{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "411c289a-e630-432c-a9c7-641638faadd9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Chapter 11: Motor control and reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaff837-9602-4b72-93b4-cbc3f4670d92",
   "metadata": {},
   "source": [
    "### Load needed modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705e45d8-a5f5-4f29-b280-f8f0e05e1307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c5b751-1c6a-4663-a47b-c255cf1d4ec9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 11.3. Formalization of reinforcement learning\n",
    "### 11.3.1. The environmental setting of a Markov decision process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ac16c9-c920-461b-a95c-18b03a6e8309",
   "metadata": {},
   "source": [
    "To introduce some fundamental concepts in reinforcement learning, we are going to use the example situation of navigating a 1-dimensional maze in order to maximize the reward obtained in one of its ends.\n",
    "\n",
    "The main two are the environmental functions of:\n",
    "- the transition function œÑ (s, a), which defines how the environment changes from one state to another when an agent takes an action\n",
    "- the reward function œÅ(s) defines the immediate numerical feedback an agent receives after taking an action in a given state\n",
    "\n",
    "There are also several helper functions:\n",
    "- calculation of the policy from a value function\n",
    "- function idx(a) to transforms the action representation a ‚àà {‚àí1, 1} to the corresponding indices idx ‚àà {0, 1}, which we need for the specific implementation of the actions in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605e024f-4f27-4f6b-9683-565b6cf67903",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reinforcement learning in 1d maze\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def tau(s,a):\n",
    "    if s==0 or s==4:\n",
    "        return(s)\n",
    "    else:\n",
    "        return(s+a)\n",
    "def rho(s,a):\n",
    "    return(s==1 and a==-1)+2*(s==3 and a==1)\n",
    "def calc_policy(Q):\n",
    "    policy=np.zeros(5)\n",
    "    for s in range(0,5):\n",
    "        action_idx=np.argmax(Q[s,:])\n",
    "        policy[s]=2*action_idx-1\n",
    "        policy[0]= policy[4]=0\n",
    "    return policy.astype(int)\n",
    "\n",
    "def idx(a):\n",
    "    return(int((a+1)/2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e259ca-0997-4b1c-802c-2a3d393d05d5",
   "metadata": {},
   "source": [
    "The expected reward (return) cannot increase without bounds. Thus, we keep the return finite by using a discounted return in which an agent values immediate reward more than a reward to be obtained far in the future. To capture this we define a **discount factor** 0 < *Œ≥* < 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5432e5f2-d160-4794-b8e2-bb2d106d5ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discount factor\n",
    "gamma=0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef477a34-d118-42ff-93ec-ebb9217bebdc",
   "metadata": {},
   "source": [
    "### 11.3.2. Model-based reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca652d5c-d5ed-492e-85d6-1db20f75a959",
   "metadata": {},
   "source": [
    "**Bellman equation**\n",
    "\n",
    "Broadly speaking, the Bellman equation describes the relationship between the value of a state and the values of its possible successor states that an agent can take.\n",
    "\n",
    "It expresses the value of a state *s* in terms of the immediate reward and the expected future value of the next state.\n",
    "\n",
    "The Bellman equation itself is not an action-value function. Instead, it is a general recursive equation used to describe the value of a state or state-action pair in terms of future rewards. The Bellman equation can be applied to both:\n",
    "\n",
    "- State-value function ùëâ(ùë†) ‚Üí The expected return when starting from state ùë† and following a policy.\n",
    "- Action-value function (Q-function) *Q(s,a)* ‚Üí The expected return when starting from state ùë†, taking action ùëé, and following a policy.\n",
    "\n",
    "It serves as the foundation for computing optimal strategies in RL by breaking down complex decision-making problems into simpler recursive relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8d27fb-42bd-47d3-991f-2e7ad3442931",
   "metadata": {},
   "source": [
    "**Reward vector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dc7b69-695a-43d5-899c-04e07d0c16d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--> Analytic solution for optimal policy')\n",
    "# Defining reward vector R\n",
    "i=0; R=np.zeros(10)\n",
    "for s in range(0,5):\n",
    "    for a in range(-1,2,2):\n",
    "        R[i]=rho(s,a)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6179f246-4f77-4cf8-9e99-35f98295869d",
   "metadata": {},
   "source": [
    "**Transition matrix**\n",
    "\n",
    "The transition matrix (Fig 11.6) is the mathematical representation of how an environment changes states when an agent takes actions (based on states, actions, and rewards).\n",
    "\n",
    "It depends on the policy, so we need to choose one. We chose the one specified on the left in Fig. 11.6 where the agent would move to the left in state s = 1 and to the right in states s = 2 and s = 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6274e039-ef1f-401a-a965-959488937a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining transition matrix\n",
    "T = np.zeros([10,10])\n",
    "T[0,0]=1; T[1,1]=1; T[2,0]=1; T[3,5]=1; T[4,2]=1\n",
    "T[5,7]=1; T[6,5]=1; T[7,9]=1; T[8,7]=1; T[9,9]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b300b911-9d14-4cef-8a6d-0b484ae7503a",
   "metadata": {},
   "source": [
    "**Q-function**\n",
    "\n",
    "The Q-function is a value function that represents the expected cumulative reward an agent can obtain by taking an action ùëé in a state ùë† and following an optimal policy thereafter.\n",
    "\n",
    "In the code below, following the 1D maze example, the first row shows the values for left movements in each state and the second row shows the values for a right movement in each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5decd8aa-5ec7-4b23-8799-e8a123ffa682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Q-function\n",
    "Q=np.linalg.inv(np.eye(10)-gamma*T) @ np.transpose(R)\n",
    "Q=np.reshape(Q,[5,2]); Q[4,0]=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e67aa7-e4aa-4a07-8953-05ed6f18a829",
   "metadata": {},
   "source": [
    "**Policy**\n",
    "\n",
    "A policy (ùúã) is a strategy or rule that an agent follows to decide which action to take in a given state. It defines the agent‚Äôs behavior at every step of interaction with the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625de895-8932-48e1-9c02-8cb92b609b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining policy based on our\n",
    "policy=calc_policy(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3804b41-6bc9-4520-ab47-17b07429491e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Q values: \\n', np.transpose(Q))\n",
    "print('policy: \\n', np.transpose(policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f2688c-e643-467e-8256-1a063f6c7ce3",
   "metadata": {},
   "source": [
    "Below we simply store the optimal Q-values for the optimal policy in this analytical solution, for later comparison with other methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a6b9c0-b409-4d5b-9091-1285153b27f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Qana=Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12134868-3f73-4d26-a01e-9b22f642f848",
   "metadata": {},
   "source": [
    "**Dynamic programming**\n",
    "\n",
    "Dynamic programming provides algorithms to compute the value function and optimal policy by iteratively solving the Bellman equation.\n",
    "It is used to for value function iteration and policy iteration and evaluation.\n",
    "\n",
    "Dynamic programming is useful when: \n",
    "- The environment‚Äôs transition probabilities and rewards are known.\n",
    "- The state space is small (DP is infeasible for large-scale problems due to high memory and computation costs).\n",
    "- Used as a baseline for approximate solutions like Q-learning or Deep RL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bce0d2f-6972-4598-bd9f-aadb04b4f9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---> Dynamic Programming')\n",
    "\n",
    "Q=np.zeros([5,2])\n",
    "for iter in range(3):\n",
    "    for s in range(0,5):\n",
    "        for a in range(-1,2,2):\n",
    "            act=int(policy[tau(s,a)])\n",
    "            Q[s, idx(a)]=rho(s,a)+gamma*Q[tau(s,a), idx(act)]\n",
    "            \n",
    "print('Q values: \\n', np.transpose(Q))\n",
    "print('policy: \\n', np.transpose(policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e7b005-63e8-427a-9eb8-f5b1e579812a",
   "metadata": {},
   "source": [
    "**Policy iteration**\n",
    "\n",
    "Policy iteration is a way of finding the policy to maximize the return.\n",
    "\n",
    "It consists in starting with an arbitrary policy and using the corresponding value function to define a new policy which is given by taking the actions from each state that will give us the best next return value.\n",
    "\n",
    "For the new policy, we can calculate the corresponding Q-function and then use this Q-function to improve the policy again (iterative process)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf6ee97-7d00-4071-a67f-113c7149d543",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---> Policy iteration')\n",
    "\n",
    "Q=np.zeros([5,2])\n",
    "policy=calc_policy(Q)\n",
    "for iter in range(3):\n",
    "    for s in range(0,5):\n",
    "        for a in range(-1,2,2):\n",
    "            act = int(policy[tau(s,a)])\n",
    "            Q[s,idx(a)]=rho(s,a)+gamma*Q[tau(s,a),idx(act)]\n",
    "    policy = calc_policy(Q)\n",
    "    \n",
    "print('Q values: \\n', np.transpose(Q))\n",
    "print('policy: \\n', np.transpose(policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20573c8b-9a90-4a93-9e25-896d40e696ed",
   "metadata": {},
   "source": [
    "**Q-iteration**\n",
    "\n",
    "Q-iteration is used in model-based RL to iteratively compute the Q-value function by leveraging a learned or known environment model.\n",
    "\n",
    "It follows the principle of value iteration, but instead of updating the state-value function, it updates the action-value function. Given a learned transition and reward function, the Q-values are updated iteratively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6476d12-8296-4921-8658-f832fa62f352",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---> Q-iteration')\n",
    "\n",
    "Q_new=np.zeros([5,2])\n",
    "Q=np.zeros([5,2])\n",
    "for iter in range(3):\n",
    "    for s in range(0,5):\n",
    "        for a in range(-1,2,2):\n",
    "            maxValue = np.maximum(Q[tau(s,a),0],Q[tau(s,a),1])\n",
    "            Q_new[s,idx(a)]=rho(s,a)+gamma*maxValue\n",
    "    Q=np.copy(Q_new)\n",
    "    \n",
    "policy=calc_policy(Q)\n",
    "print('Q values: \\n', np.transpose(Q))\n",
    "print('policy: \\n', np.transpose(policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89d4000-f4fc-4394-9c8c-dff1e352c408",
   "metadata": {},
   "source": [
    "### 11.3.3. Model-free reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06205090-9925-4919-896f-ddb1f7b8b0bd",
   "metadata": {},
   "source": [
    "In model-free RL, we combine sampling with RL by exploring the environment.\n",
    "\n",
    "Similarly to model-based RL, we want to minimize the difference between the left- and the right-hand sides of the Bellman equation.\n",
    "\n",
    "But we cannot calculate the right-hand side because we don't know the transition or the reward functions (which we do know in model-based RL).\n",
    "\n",
    "**SARSA**\n",
    "\n",
    "Therefore, we use the SARSA (state->action->reward->state->action) algorithm.\n",
    "\n",
    "SARSA consists in taking a step based on our policy, and observe a reward ($r_{i+1}$) and a next state ($s_{i+1}$).\n",
    "\n",
    "Since this is only one step and sample, we should use this update of the value function only with a small learning rate (*Œ±*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764bfc63-c445-453b-9125-bdd81921c289",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---> SARSA')\n",
    "\n",
    "Q=np.zeros([5,2])\n",
    "error = []\n",
    "alpha = 1\n",
    "for trial in range(200):\n",
    "    policy=calc_policy(Q)\n",
    "    s=2\n",
    "    for t in range(0,5):\n",
    "        a=policy[s]\n",
    "        if np.random.rand()<0.1: a=-a #epsilon greedy\n",
    "        a2=idx(policy[tau(s,a)])\n",
    "        TD=rho(s,a)+gamma*Q[tau(s,a),a2]-Q[s,idx(a)]\n",
    "        Q[s, idx(a)]=Q[s, idx(a)]+alpha*TD\n",
    "        s=tau(s,a)\n",
    "    error.append(np.sum(np.sum(np.abs(np.subtract(Q,Qana)))))\n",
    "\n",
    "print('Q values: \\n', np.transpose(Q))\n",
    "print('policy: \\n', np.transpose(policy))\n",
    "plt.figure(); plt.plot(error)\n",
    "plt.xlabel('iteration'); plt.ylabel('error (Q - Qana)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7042bc-d659-40e7-b37e-cdc841e8bb73",
   "metadata": {},
   "source": [
    "**Q-learning**\n",
    "\n",
    "Q-learning is a model-free RL algorithm that learns the optimal q-function without requiring prior knowledge of the environment's transition probabilities.\n",
    "\n",
    "It is an off-policy, value-based method that uses Q-values (action-value function) to estimate the expected cumulative reward for taking an action in a given state. As a result, it updates Q-values iteratively using the Bellman equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bc92f6-2466-4074-bbb1-174e138e6148",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--->Q-Learning:')\n",
    "\n",
    "Q=np.zeros([5,2])\n",
    "alpha=1\n",
    "error=[]\n",
    "for trial in range(200):\n",
    "    policy=calc_policy(Q)\n",
    "    s=2\n",
    "    for t in range(0,5):\n",
    "        a=policy[s]\n",
    "        if np.random.rand()<0.2: a =-a #epsilon greedy\n",
    "        TD=rho(s,a)+gamma*np.maximum(Q[tau(s,a),0],Q[tau(s,a),1])-Q[s,idx(a)]\n",
    "        Q[s,idx(a)]=Q[s,idx(a)]+alpha*TD\n",
    "        Q[0]=0; Q[4]=0\n",
    "        s=tau(s,a)\n",
    "    error.append(np.sum(np.sum(np.abs(np.subtract(Q,Qana)))))\n",
    "    \n",
    "print('Q values: \\n', np.transpose(Q))\n",
    "print('policy: \\n', np.transpose(policy))\n",
    "plt.plot(error,'r')\n",
    "plt.xlabel('iteration'); plt.ylabel('error (Q - Qana)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31a311-d39e-4b7e-b75b-5856026cf758",
   "metadata": {},
   "source": [
    "**TD(Œª)**\n",
    "\n",
    "TD(Œª), or temporal difference with eligibility traces, is controlled by a trace decay parameter ùúÜ, which allows for both short-term and long-term credit assignment.\n",
    "\n",
    "An eligibility trace keeps track of how much credit each state (or state-action pair) should receive for future rewards. It works like a memory that assigns weight to past states, depending on how recent they were visited.\n",
    "\n",
    "The value function update of TD(Œª) depends on Œª:\n",
    "- **Œª**=0: Equivalent to TD(0) (only updates based on the most recent step).\n",
    "- **Œª**=1: Equivalent to Monte Carlo (updates values using full-episode returns).\n",
    "- 0<**Œª**<1: A trade-off, balancing short-term and long-term learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913e2a39-14cf-4a0e-a34a-d37cf989d43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---> TD(lambda) for Q-learning')\n",
    "\n",
    "Q=np.zeros([5,2])\n",
    "alpha=1\n",
    "error=[]\n",
    "eligibility=np.zeros([5,2])\n",
    "lam=0.7\n",
    "\n",
    "for trial in range(200):\n",
    "    policy=calc_policy(Q)\n",
    "    s=2\n",
    "    for t in range(0,5):\n",
    "        a=policy[s]\n",
    "        if np.random.rand()<0.1: a=-a #epsilon greedy\n",
    "        TD=rho(s,a)+gamma * np.maximum(Q[tau(s,a),0], Q[tau(s,a),1])-Q[s,idx(a)]\n",
    "        eligibility*=gamma*lam\n",
    "        eligibility[s,idx(a)]=1\n",
    "        for si in range(1,4):\n",
    "            for ai in range(2):\n",
    "                Q[si, ai]=Q[si, ai]+alpha*TD*eligibility[si,ai]\n",
    "        Q[0]=0; Q[4]=0;\n",
    "        s=tau(s,a)\n",
    "        error.append(np.sum(np.sum(np.abs(np.subtract(Q,Qana)))))\n",
    "        \n",
    "print('Q values: \\n', np.transpose(Q))\n",
    "print('policy: \\n', np.transpose(policy))\n",
    "plt.plot(error, 'r');\n",
    "plt.xlabel('iteration'); plt.ylabel('error')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606bd9d8-288e-406b-9b09-d640f62e8d32",
   "metadata": {},
   "source": [
    "## 11.4. Deep reinforcement learning\n",
    "### 11.4.1. Value-function approximation with ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dae5a2-e72a-4be0-b532-47a521b56c5a",
   "metadata": {},
   "source": [
    "Given that the \"cartoonish\" way of approximating a function with tabular methods is unfeasible in real-life applications, we can make use of more complex function approximations.\n",
    "\n",
    "There are simple methods, such as linear regression, that work fine as a baseline method, since they are also tractable analytically. \n",
    "\n",
    "However, since many real-world applications are highly non-linear, ANN become more useful.\n",
    "\n",
    "**Neurally-fitted Q-iteration (NFQ)** is an example one example of the algorithms for value-function approximation with ANN (Fig 11.11 B). It consists in training a network that provides the Q-values for all the possible actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1115262c-2af8-4342-a603-21f9881267fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import models, layers, optimizers\n",
    "\n",
    "def tau(s,a):\n",
    "    if (s[0] and s[4]) == 0:\n",
    "        s=np.roll(s,a)\n",
    "    return s\n",
    "def rho(s):\n",
    "        return ((s[0]==1)+2*(s[4]==1))\n",
    "def terminal_state(s):\n",
    "        return (s[0]==1 or s[4]==1)\n",
    "\n",
    "gamma=0.5\n",
    "invT=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1c94ec-c54e-4211-8d48-2f94bbf2dd71",
   "metadata": {},
   "source": [
    "Here, we specify a **small dense network** with 5 inputs for the state vector, 10 hidden nodes, and 2 output nodes, each representing the Q-value for each possible action, that of going left or right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab19a9a1-5cd7-49dc-a07e-a4366e6db1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the network\n",
    "inputs = layers.Input(shape=(5,))\n",
    "h = layers.Dense(10, activation='relu')(inputs)\n",
    "outputs = layers.Dense(2, activation='linear')(h)\n",
    "\n",
    "model = models.Model(inputs=inputs, outputs=outputs)\n",
    "RMSprop = optimizers.RMSprop(learning_rate=0.01)\n",
    "model.compile(loss='mse', optimizer=RMSprop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489f1494-c739-421e-94ad-fe36b66493ee",
   "metadata": {},
   "source": [
    "From the current state we use the network to predict the corresponding *Q-value* and then move one step ahead to calculate the target for the gradient learning as *r + Q(next_s)*. The network is then updated right away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f767e24-9e10-48bf-861f-6b7ee3459bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for trial in range(400):\n",
    "    s = np.array([0,0,1,0,0])\n",
    "    for t in range(0,5):\n",
    "        if terminal_state(s): break\n",
    "        if trial > 30 and invT > 0.1: invT -= 0.001\n",
    "        prediction=model.predict(s.reshape(1,5), steps=1, verbose=0)\n",
    "        aidx=np.argmax(prediction)\n",
    "        if np.random.rand() < invT: aidx=1-aidx\n",
    "        a=2*aidx-1\n",
    "        next_s = tau(s,a)\n",
    "        if terminal_state(next_s):\n",
    "            y = rho(next_s)\n",
    "        else:\n",
    "            y = gamma*np.max(model.predict(next_s.reshape(1,5), steps=1, verbose=0))\n",
    "            prediction[0, aidx]=y\n",
    "            model.fit(s.reshape(1,5), prediction, epochs=1, verbose=0)\n",
    "            s = np.copy(next_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310ba6fc-38b2-4f4d-b072-71c372f933d1",
   "metadata": {},
   "source": [
    "After the exploration we can evaluate the final policy and value functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e110587c-2b17-43a2-a843-e7907f2eaa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "for trial in range(400):\n",
    "    s = np.array([0,0,1,0,0])\n",
    "    for t in range(0,5):\n",
    "        if terminal_state(s): break\n",
    "        if trial > 30 and invT > 0.1: invT -= 0.001\n",
    "        prediction=model.predict(s.reshape(1,5), steps=1, verbose=0)\n",
    "        aidx=np.argmax(prediction)\n",
    "        if np.random.rand() < invT: aidx=1-aidx\n",
    "        a=2*aidx-1\n",
    "        next_s = tau(s,a)\n",
    "        if terminal_state(next_s):\n",
    "            y = rho(next_s)\n",
    "        else:\n",
    "            y = gamma*np.max(model.predict(next_s.reshape(1,5), steps=1, verbose=0))\n",
    "            prediction[0, aidx]=y\n",
    "            model.fit(s.reshape(1,5), prediction, epochs=1, verbose=0)\n",
    "            s = np.copy(next_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788cd337-0e1d-4ebf-ab5d-cdcebf22fe43",
   "metadata": {},
   "source": [
    "### 11.4.7. Neural implementations of TD learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281f61b1-83df-4105-8e1a-4c17498ba499",
   "metadata": {},
   "source": [
    "TD learning updates value estimates based on the difference between consecutive time steps, rather than waiting for the final reward at the end of an episode. The key idea is to learn from incomplete episodes by bootstrapping (updating estimates using other estimates). Given this incremental feature of updating its values at every step, it becomes more sample-efficient than e.g., Monte Carlo methods.\n",
    "\n",
    "TD learning provides a computational model of reward-based learning that aligns to an extent with biological reinforcement mechanisms in behavior (e.g., Rescorla-Wagner) and the brain (e.g., Schulz et al., 1997).\n",
    "\n",
    "The dopamine system plays a crucial role in encoding prediction errors, while the basal ganglia and cortex help store and update learned values, enabling adaptive behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eab9de-1f02-4e1f-8bd0-992bcc61bcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma=0\n",
    "w=np.zeros(10)\n",
    "Z=np.zeros((100,6)); rhatREC=np.zeros(100)\n",
    "x=np.random.rand(10,6)\n",
    "r=np.array([0,0,1,0,0.5,0])\n",
    "\n",
    "for trial in range(100):\n",
    "    V=0\n",
    "    for t in range(1,6):\n",
    "        Vlast=V\n",
    "        V=w@x[:,t]\n",
    "        rhat=r[t-1]+gamma*V-Vlast\n",
    "        w=w+0.1*rhat*x[:,t-1]\n",
    "        Z[trial,t-1]=Vlast\n",
    "        rhatREC[trial]=rhatREC[trial]+rhat\n",
    "plt.figure(); plt.plot(Z[-1,:])\n",
    "plt.figure(); plt.plot(rhatREC/5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5421ab8-730f-4894-968d-20ebcde27a32",
   "metadata": {},
   "source": [
    "TD learning has the ability of **transferring the reward response to earlier predictive cues** (conditioned stimuli), and such characteristic neural responses where discovered by Schulz et al. (1997) in dopaminergic neurons in some areas of the midbrain called the Ventral Tegmental Area (VTA) and the Substantia Nigra (SN).\n",
    "\n",
    "Such reward-response transfer to the conditioned stimulus is simulated in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a9ad1c-0b83-4ab1-a182-469d928f648a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma=1\n",
    "w=np.zeros(20); Z=np.zeros((30,21))\n",
    "\n",
    "for trial in range(30):\n",
    "    x=np.zeros(20); V=0;\n",
    "    for t in range(1,20):\n",
    "        xlast=np.copy(x)\n",
    "        Vlast=V\n",
    "        x[t]=x[t-1] # shift visual memory\n",
    "        x[t-1]=0\n",
    "        if t==2: x[t]=1 # vis onset\n",
    "        print(t,x) \n",
    "        V=w@x\n",
    "        rhat=gamma*V-Vlast\n",
    "        w=w+1*rhat*xlast\n",
    "        Z[trial,t]=rhat\n",
    "    rhat=1-V # last time gets reward\n",
    "    w=w+1*rhat*x\n",
    "    Z[trial, t+1]=rhat\n",
    "plt.imshow(Z, cmap='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227555df-64bc-41d6-9efd-b1e1e1366afd",
   "metadata": {},
   "source": [
    "## Homework\n",
    "Due Monday, Mar. 31, 13.00. Send to jperez@bcbl.eu in either .doc, .txt, or just by inserting it in the markdown cell below and sending me a copy of this notebook\n",
    "1. Do you see the fundamentals of reinforcement learning (e.g., state, reward, actions) as more based on behavioral or neurobiological foundations? Or both? Develop a bit your response ;)\n",
    "2. What elements for learning do you see that temporal difference includes? Do you see those elements leaning more towards the reward-driven learning or also as elements for predictive processing/learning? It can be just your impression and thoughts from a first glance at the chapter! \n",
    "3. Can you speculate on a potential model that uses reinforcement learning to explore/simulate a language process (e.g., vocabulary learning, speech tracking, morphological processing?)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
